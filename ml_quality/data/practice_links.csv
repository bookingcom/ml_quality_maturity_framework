practice,link
mimic production setting in offline evaluation,""
fast development lifecycle feedback loops for easier debugging,""
unified environment for all lifecycle steps,""
understand AB assumption violations and use alternatives,""
automate the ML lifecycle,""
prevent consequences of using a model as input to another,""
monitor feature parity,""
monitor feature drift,""
offline metric correlates with production metric,""
error analysis,""
use of containarized environment,""
turn experimental code into production code,""
ownership,""
keep model up to date by retraining it with new labelled data points,""
achieve feature parity,""
AB test all model changes,""
code versioning,""
keep historical features up to date,""
model versioning,""
"reuse data, supported internal tooling and solutions",""
establish clear success metric before model design,""
canary deployment (gradual rollouts),""
do not test model which is not promising offline,""
logging of metadata and artifacts,""
data versioning,""
peer code review,""
code modularity and reusability,""
documentation,""
remove redundant features,""
automated tests,""
impact of model staleness is known,""
model performance monitoring,""
alerting,""
ML System Brainstorms,""
"register the model in a centralized registry, like ML Portal",""
"use a unified code style, like PEP8",https://peps.python.org/pep-0008/
Input sanitization and bot detection,""
Request a threat model session,""
Input data validation,""